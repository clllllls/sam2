scratch:
  resolution: 512
  train_batch_size: 4
  num_train_workers: 4
  max_num_objects: 3
  base_lr: 5.0e-05
  vision_lr: 3.0e-05
  phases_per_epoch: 1
  num_epochs: 50
dataset:
  img_folder: D:/sam2-main/dataset/images
  gt_folder: D:/sam2-main/dataset/json
  file_list_txt: null
image_transforms:
  train_transforms:
  - _target_: training.dataset.transforms.ComposeAPI
    transforms:
    - _target_: training.dataset.transforms.RandomHorizontalFlip
      consistent_transform: true
    - _target_: training.dataset.transforms.RandomAffine
      degrees: 25
      shear: 20
      image_interpolation: bilinear
      consistent_transform: true
    - _target_: training.dataset.transforms.RandomResizeAPI
      sizes: ${scratch.resolution}
      square: true
      consistent_transform: true
    - _target_: training.dataset.transforms.ColorJitter
      consistent_transform: true
      brightness: 0.1
      contrast: 0.03
      saturation: 0.03
      hue: null
    - _target_: training.dataset.transforms.RandomGrayscale
      p: 0.05
      consistent_transform: true
    - _target_: training.dataset.transforms.ToTensorAPI
    - _target_: training.dataset.transforms.NormalizeAPI
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
trainer:
  _target_: training.trainer.Trainer
  mode: train_only
  max_epochs: ${times:${scratch.num_epochs},${scratch.phases_per_epoch}}
  accelerator: cuda
  seed_value: 123
  model:
    _target_: training.model.sam2.SAM2Train
    image_encoder:
      _target_: sam2.modeling.backbones.image_encoder.ImageEncoder
      scalp: 1
      trunk:
        _target_: sam2.modeling.backbones.hieradet.Hiera
        embed_dim: 112
        num_heads: 2
        drop_path_rate: 0.1
      neck:
        _target_: sam2.modeling.backbones.image_encoder.FpnNeck
        position_encoding:
          _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
          num_pos_feats: 256
          normalize: true
          scale: null
          temperature: 10000
        d_model: 256
        backbone_channel_list:
        - 896
        - 448
        - 224
        - 112
        fpn_top_down_levels:
        - 2
        - 3
        fpn_interp_model: nearest
    memory_encoder:
      _target_: sam2.modeling.memory_encoder.MemoryEncoder
      out_dim: 64
      position_encoding:
        _target_: sam2.modeling.position_encoding.PositionEmbeddingSine
        num_pos_feats: 64
        normalize: true
        scale: null
        temperature: 10000
      mask_downsampler:
        _target_: sam2.modeling.memory_encoder.MaskDownSampler
        kernel_size: 3
        stride: 2
        padding: 1
      fuser:
        _target_: sam2.modeling.memory_encoder.Fuser
        layer:
          _target_: sam2.modeling.memory_encoder.CXBlock
          dim: 256
          kernel_size: 7
          padding: 3
          layer_scale_init_value: 1.0e-06
          use_dwconv: true
        num_layers: 2
    image_size: ${scratch.resolution}
    sigmoid_scale_for_mem_enc: 20.0
    sigmoid_bias_for_mem_enc: -10.0
    use_mask_input_as_output_without_sam: true
    use_high_res_features_in_sam: true
    multimask_output_in_sam: true
    iou_prediction_use_sigmoid: true
    directly_add_no_mem_embed: true
    prob_to_use_pt_input_for_train: 0.7
    prob_to_use_box_input_for_train: 0.7
    prob_to_sample_from_gt_for_train: 0.2
    num_correction_pt_per_frame: 5
    num_init_cond_frames_for_train: 1
    num_init_cond_frames_for_eval: 1
  data:
    train:
      _target_: training.dataset.sam2_datasets.TorchTrainMixedDataset
      phases_per_epoch: ${scratch.phases_per_epoch}
      batch_sizes:
      - ${scratch.train_batch_size}
      datasets:
      - _target_: training.dataset.vos_dataset.VOSDataset
        transforms: ${image_transforms.train_transforms}
        training: true
        video_dataset:
          _target_: training.dataset.bmp_raw_dataset.BMPRawDataset
          img_folder: ${dataset.img_folder}
          gt_folder: ${dataset.gt_folder}
          file_list_txt: ${dataset.file_list_txt}
        sampler:
          _target_: training.dataset.vos_sampler.RandomUniformSampler
          num_frames: 1
          max_num_objects: ${scratch.max_num_objects}
        multiplier: 1
      dataset_prob:
      - 1.0
      shuffle: true
      num_workers: ${scratch.num_train_workers}
      pin_memory: true
      drop_last: true
      collate_fn:
        _target_: training.utils.data_utils.collate_fn
        _partial_: true
        dict_key: all
  optim:
    amp:
      enabled: true
      amp_dtype: bfloat16
    optimizer:
      _target_: torch.optim.AdamW
    gradient_clip:
      _target_: training.optimizer.GradientClipper
      max_norm: 0.1
      norm_type: 2
    param_group_modifiers:
    - _target_: training.optimizer.layer_decay_param_modifier
      _partial_: true
      layer_decay_value: 0.9
      apply_to: image_encoder.trunk
      overrides:
      - pattern: '*pos_embed*'
        value: 1.0
    options:
      lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CosineParamScheduler
          start_value: ${scratch.base_lr}
          end_value: ${divide:${scratch.base_lr},10}
      - scheduler:
          _target_: fvcore.common.param_scheduler.CosineParamScheduler
          start_value: ${scratch.vision_lr}
          end_value: ${divide:${scratch.vision_lr},10}
        param_names:
        - image_encoder.*
      weight_decay:
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.1
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.0
        param_names:
        - '*bias*'
        module_cls_names:
        - torch.nn.LayerNorm
  loss:
    all:
      _target_: training.loss_fns.MultiStepMultiMasksAndIous
      weight_dict:
        loss_mask: 10
        loss_dice: 1
        loss_iou: 1
        loss_class: 1
      supervise_all_iou: true
      iou_use_l1_loss: true
  distributed:
    backend: gloo
    find_unused_parameters: true
    timeout_mins: 30
  logging:
    tensorboard_writer:
      _target_: training.utils.logger.make_tensorboard_logger
      log_dir: ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
      should_log: true
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10
  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 5
    model_weight_initializer:
      _partial_: true
      _target_: training.utils.checkpoint_utils.load_state_dict_into_model
      strict: false
      ignore_unexpected_keys:
      - no_obj_ptr
      - no_obj_embed_spatial
      - mask_downsample.*
      - memory_attention.*
      - sam_mask_decoder.obj_score_token.*
      - sam_mask_decoder.pred_obj_score_head.*
      - obj_ptr_proj.*
      - obj_ptr_tpos_proj.*
      ignore_missing_keys: null
      state_dict:
        _target_: training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: ../checkpoints/sam2.1_hiera_base_plus.pt
        ckpt_state_dict_keys:
        - model
launcher:
  num_nodes: 1
  gpus_per_node: 1
  experiment_log_dir: ./logs/image_training
submitit:
  partition: null
  account: null
  qos: null
  cpus_per_task: 4
  use_cluster: false
  timeout_hour: 24
  name: sam2-image-training
  port_range:
  - 10000
  - 20000
